# Enterprise AI Assistant (Ollama + Pinecone)

An endâ€‘toâ€‘end **enterprise knowledge assistant** for a SaaS product â€“ your own Jarvis for the enterprise.  
It uses a **selfâ€‘hosted LLM (Ollama / LLaMA)**, **Pinecone** as a vector database, and a modern **React + Vite + Tailwind** chatbot UI.

---

## 1. Project Structure

```text
enterprise-ai-assistant/
â”œâ”€â”€ backend/              # Node.js + Express RAG API
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ollamaService.js   # Calls local Ollama (LLaMA + embeddings)
â”‚   â”‚   â”‚   â”œâ”€â”€ pineconeService.js # Pinecone client + vector ops
â”‚   â”‚   â”‚   â””â”€â”€ ragService.js      # Retrieval-Augmented Generation orchestration
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.js            # Chat endpoints with sessions & rate limit
â”‚   â”‚   â”‚   â””â”€â”€ knowledge.js       # Knowledge management CRUD + search
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ sessionManager.js  # Inâ€‘memory conversation store
â”‚   â”‚   â””â”€â”€ server.js              # Express app, health checks, startup
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/            # React + Vite + Tailwind chatbot UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.jsx  # Main chat experience
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageList.jsx    # Message rendering, markdown, sources
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInput.jsx      # Input box with validation
â”‚   â”‚   â”‚   â””â”€â”€ KnowledgePanel.jsx # Admin knowledge base panel
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js             # Axios wrapper for backend API
â”‚   â”‚   â”œâ”€â”€ App.jsx                # Layout, health status, dark mode
â”‚   â”‚   â””â”€â”€ main.jsx               # Vite/React entry (standard)
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ src/index.css
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## 2. Prerequisites

- **Node.js** â‰¥ 18 (for both frontend and backend)
- **npm** or **yarn**
- **Ollama** installed and running locally  
  - Install from `https://ollama.com/`
  - Pull required models:
    ```bash
    ollama pull llama3.2
    ollama pull nomic-embed-text
    ```
- **Pinecone** account and API key (`https://www.pinecone.io/`)
  - Create an index (e.g. `enterprise-assistant`) with dimension **384** for `nomic-embed-text`.

---

## 3. Environment Configuration

### 3.1 Backend (`backend/.env`)

Use `.env.example` as a template:

```env
PORT=3000
OLLAMA_BASE_URL=http://localhost:11434
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=enterprise-assistant
PINECONE_ENVIRONMENT=your_environment
NODE_ENV=development
```

> The backend exposes REST endpoints for chat, knowledge management, and a health check at `/api/health`.

### 3.2 Frontend (`frontend/.env`)

```env
VITE_API_URL=http://localhost:3000
VITE_APP_NAME=Enterprise AI Assistant
```

---

## 4. Installation & Running

From the project root (`enterprise-ai-assistant`):

### 4.1 Install dependencies

```bash
# Backend
cd backend
npm install

# Frontend
cd ../frontend
npm install
```

### 4.2 Start services

1. **Start Ollama** (if not already running) and ensure:
   - `llama3.2` and `nomic-embed-text` are available.

2. **Start backend API**

```bash
cd backend
npm run dev
# or: npm start
```

The backend will:
- Validate connectivity to **Ollama** and **Pinecone**
- Log available Ollama models
- Serve:
  - `POST /api/chat` â€“ chat completion with RAG
  - `GET /api/chat/history/:conversationId`
  - `DELETE /api/chat/:conversationId`, `POST /api/chat/clear-all`
  - `POST /api/knowledge/add`, `/bulk-add`
  - `GET /api/knowledge/search`, `/list`
  - `DELETE /api/knowledge/:id`
  - `GET /api/health` â€“ health + connectivity info

3. **Start frontend UI**

```bash
cd ../frontend
npm run dev
```

Frontend runs on `http://localhost:5173` by default and talks to the backend at `VITE_API_URL`.

---

## 5. How It Works (High Level)

- **Chat Flow**
  1. User sends a message from the React chat UI (`ChatInterface`).
  2. Frontend calls `POST /api/chat` with optional `conversationId`.
  3. Backend:
     - Uses **`sessionManager`** to store conversation context in-memory (Map + auto-cleanup).
     - Calls **RAG service**:
       - Creates embedding with `nomic-embed-text` via Ollama.
       - Queries **Pinecone** for similar documents.
       - Builds a system + context prompt and calls **llama3.2** via Ollama.
     - Returns AI answer, sources, confidence, and a `conversationId`.
  4. UI renders assistant reply plus source snippets and confidence.

- **Knowledge Management**
  - Admin uses **Knowledge Panel**:
    - **Add Knowledge**: single or bulk documents (separated by `---`), with category and source.
    - **View All**: paginated list of vectors stored in Pinecone with delete actions.
    - **Search**: semantic search using the same embedding model; results show text, score, and category.

---

## 6. Frontend Features

- **Chat Experience**
  - Responsive, mobileâ€‘friendly layout using **TailwindCSS**.
  - Chat bubbles with roleâ€‘based styling, timestamps, and markdown rendering (`react-markdown`).
  - Typing indicator, copyâ€‘toâ€‘clipboard, Enter/Shift+Enter behavior, validation, character limits.
  - Conversation + `conversationId` persisted in `localStorage`.

- **Knowledge Admin Panel**
  - Tabs for **Add Knowledge**, **View All**, and **Search**.
  - Bulk add via `---` separators.
  - Paginated table view with skeleton loaders, delete confirmation modal.
  - Search with similarity score visualizations.
  - Toasts via **react-hot-toast**.

- **App Shell**
  - Top nav with health indicator for **Ollama** and **Pinecone** (polling every 30s).
  - Dark mode toggle (saved in `localStorage`).
  - Framerâ€‘motion animations for sidebar and transitions.
  - Simple error boundary wrapper.

---

## 7. Useful Commands

From `backend/`:

```bash
npm run dev     # Start backend in watch mode
npm start       # Start backend normally
```

From `frontend/`:

```bash
npm run dev     # Start Vite dev server
npm run build   # Production build
npm run preview # Preview production build
```

---

## 8. Notes & Next Steps

- This project uses **inâ€‘memory sessions**; for production you should move session storage to Redis or another durable store.
- Ensure your **Pinecone index dimension** matches the embedding model (384 for `nomic-embed-text`).
- For enterprise use, add authentication and roleâ€‘based access for the knowledge admin panel.
- Error handling and logging are wired in, but you can integrate with observability tools (e.g. Datadog, Sentry) for production.

This repository is designed to be **simple to run**, yet **close to a real enterprise RAG assistant**: selfâ€‘hosted LLM, vector database, and a clean chatbot UI. Enjoy experimenting and extending it. ðŸš€

